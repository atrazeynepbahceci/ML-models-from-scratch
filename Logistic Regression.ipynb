{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nrsLg44JROLu"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nss-QS0J8a1V"
   },
   "source": [
    "# Fetching, Scaling & Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AGFzACtQRkX1"
   },
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    \n",
    "    df=pd.read_csv(\"titanicdata.csv\")\n",
    "    \n",
    "    global x_df,y_df\n",
    "    x_df = df.drop('Survived',axis=1)\n",
    "    y_df = df['Survived']\n",
    "    \n",
    "    if df.isna().any().any()==True:\n",
    "        print(\"There are missing values in the dataset!\") #to check beforehand\n",
    "    \n",
    "    return df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "rGyR9MckRvgL",
    "outputId": "941a547f-d2ea-45b8-ee64-773b3c23df69"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass  Sex   Age\n",
       "0         0       3    2  22.0\n",
       "1         1       1    1  38.0\n",
       "2         1       3    1  26.0\n",
       "3         1       1    1  35.0\n",
       "4         0       3    2  35.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "bQSCx00cRmTR"
   },
   "outputs": [],
   "source": [
    "def scale_x(x):\n",
    "    #does min-max scaling\n",
    "    \n",
    "    global scaled_x\n",
    "    scaled_x = pd.DataFrame()\n",
    "    \n",
    "    for i in range(x_df.shape[1]): #x.shape[1] is the number of features (3 for the titanic dataset)\n",
    "        scaled_feature=[]\n",
    "        feature=x_df.iloc[:,i]\n",
    "        min_element=min(feature)\n",
    "        max_element=max(feature)\n",
    "        for item in x_df.iloc[:,i]:\n",
    "            scaled_item=(item-min_element)/(max_element-min_element)\n",
    "            scaled_feature.append(scaled_item)\n",
    "        scaled_x[x_df.columns[i]]=scaled_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o5tLsl2JRx0A"
   },
   "outputs": [],
   "source": [
    "scale_x(x_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "tPjAlofTRonw"
   },
   "outputs": [],
   "source": [
    "def split_data(x,y):\n",
    "    \n",
    "    global x_train, x_valid, x_test, y_train, y_valid, y_test\n",
    "    \n",
    "    x_remain, x_test, y_remain, y_test = train_test_split(x_df,y_df, test_size=0.2, random_state=42) \n",
    "    #20% of the original dataset is seperated as test data\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x_remain,y_remain,test_size=0.25,random_state=42)\n",
    "    #80% of the original data remains. 25% of the remaining data, which equals to 20% of the \n",
    "    #original data  (because (0.8)*(0.25)=0.2) is seperated as validation data.\n",
    "    #Remaining data, which is 60% of the original data, is set as training data.\n",
    "    \n",
    "    x_train=x_train.reset_index(drop=True) # because indexes got unordered in splitting\n",
    "    x_valid=x_valid.reset_index(drop=True)\n",
    "    x_test=x_test.reset_index(drop=True)\n",
    "    y_train=y_train.reset_index(drop=True)\n",
    "    y_valid=y_valid.reset_index(drop=True)\n",
    "    y_test=y_test.reset_index(drop=True)\n",
    "    \n",
    "    #check for splitting \n",
    "    if not (len(x)*0.6 -1) <= x_train.shape[0] <= (len(x)*0.6 +1): #+1 -1 in case there is odd number of data\n",
    "        print(\"Error in splitting!\")\n",
    "    if not (len(x)*0.2 -1) <= x_test.shape[0] <= (len(x)*0.2 +1):\n",
    "        print(\"Error in splitting!\")\n",
    "    if not (len(x)*0.2 -1) <= x_valid.shape[0] <= (len(x)*0.2 +1): \n",
    "        print(\"Error in splitting!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UAY2sGKIRzbn"
   },
   "outputs": [],
   "source": [
    "split_data(scaled_x,y_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CwOha_Yh8ieq"
   },
   "source": [
    "# Logistic Regression Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "LgkTZb-DRt2W"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "  #defining a sigmoid function helps simplify the rest of the calculations\n",
    "    return np.exp(z) / (1 + np.exp(z)) #sigmoid formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ziqu2jDeSHsr"
   },
   "outputs": [],
   "source": [
    "def gradients(x, y, y_pred):\n",
    "  #calculated the gradients\n",
    "  \n",
    "    num_of_instances = x.shape[0]\n",
    "\n",
    "    #by the gradient descent formula\n",
    "    d_weights = (1/num_of_instances ) * np.dot(x.T, (y_pred-y)) #np.dot gives product of two vectors\n",
    "    d_bias = (1/num_of_instances ) * np.sum(y_pred - y)\n",
    "\n",
    "    return d_weights, d_bias #will be used in training and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fz4n-I_JSFs0"
   },
   "outputs": [],
   "source": [
    "def loss_function(y, y_pred):\n",
    "  #calculated the loss for each predicted instance\n",
    "    loss = - np.mean(y * (np.log(y_pred)) - (1-y) * np.log(1-y_pred)) #by formula\n",
    "    return loss #will be used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mTMcmDGnSMxs"
   },
   "outputs": [],
   "source": [
    "def train(x, y, learning_rate=0.01, max_iter=1000): #just arbitrary inital parameter values\n",
    "  #epsilon here is sort of a stopping condition\n",
    "  #In detail, if the change in every descent is smaller than the epsilon, the model is not\n",
    "  #improving much, terminate \n",
    "    global weights,bias,y_pred,loss_list\n",
    "\n",
    "    num_of_instances, num_of_features = x.shape\n",
    "    weights = np.zeros((num_of_features, 1)) #for the starting value\n",
    "    bias = 0 #for the starting value\n",
    "    \n",
    "    global loss_list, change_per_iteration #made these global to plot later on\n",
    "    change_per_iteration = []\n",
    "    loss_list = []\n",
    "\n",
    "    y = np.array(y_train).reshape(num_of_instances, 1)\n",
    "\n",
    "    for i in range(max_iter):            \n",
    "\n",
    "        y_pred = sigmoid(np.dot(x, weights) + bias) #by formula\n",
    "        \n",
    "        d_weights, d_bias = gradients(x, y, y_pred) #fetches the parameters again each iteration\n",
    "            \n",
    "        weights -= learning_rate * d_weights #updates parameters, again just re-wrote the formula in Python format\n",
    "      \n",
    "        bias -= learning_rate * d_bias #updates parameters\n",
    "\n",
    "        change=learning_rate * d_weights #created a variable for change to plot later on\n",
    "        change_per_iteration.append(sum(change)) #storing changes to plot later on\n",
    "                                                #got the sum, because I will create a 2D graph\n",
    "                                                #seeing convergence in the graph is enough, \n",
    "                                                #the sum will be able to show that\n",
    "\n",
    "        loss = loss_function(y, sigmoid(np.dot(x, weights) + bias)) #gets the loss for the corresponding iter\n",
    "        \n",
    "        #for early termination\n",
    "        termination = 0 \n",
    "        if len(loss_list) > 0 and loss_list[-1] == loss:\n",
    "            termination += 1\n",
    "            if termination== 100:\n",
    "                return weights, bias, loss_list\n",
    "\n",
    "        \n",
    "        loss_list.append(loss)#stores losses to plot later on\n",
    "\n",
    "  \n",
    "    return weights, bias, loss_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "HnDWcoYLSl2H"
   },
   "outputs": [],
   "source": [
    "def predict(x, weights, bias):\n",
    "\n",
    "    predictions = sigmoid(np.dot(x, weights) + bias) #by formula\n",
    "    \n",
    "    global pred_class #global so that I can plot them later on, outside the function\n",
    "    pred_class = [] #stores predicted labels to calculate accuracy and to plot later on\n",
    "    pred_class = [1 if i > 0.5 else 0 for i in predictions]\n",
    "    return np.array(pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZGowAJe8vhB"
   },
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_function():\n",
    "  #not cross validation etc, but since the data is low-dimensional, this approach should work\n",
    "    \n",
    "    global val_accs, valid_df\n",
    "\n",
    "    #lists will store the corresponding values for each learning rate-max iteration pair\n",
    "    val_accs = [] \n",
    "    val_losses = [] \n",
    "    weights_list = [] \n",
    "    bias_list = []\n",
    "\n",
    "    iterations = [2500,5000,7500, 10000, 12500,15000,17500,20000,22500,25000] #max_iter values to validate\n",
    "    learning_rates = [0.0001, 0.001, 0.01] # learning_rate values to validate\n",
    "\n",
    "    valid = []\n",
    "    for iter in iterations:\n",
    "        for lr in learning_rates: #nested for loop ensures both parameters are validated\n",
    "                                  #simultaneously, not sequentially since they may affect each other\n",
    "            weights, bias, loss = train(x_train, y_train, learning_rate=lr,max_iter=iter )\n",
    "            y_pred = predict(x_valid, weights, bias)\n",
    "            acc = accuracy_score(y_valid, y_pred)\n",
    "\n",
    "            weights_list.append(weights)\n",
    "            bias_list.append(bias)\n",
    "            val_accs.append(acc)\n",
    "            val_losses.append(loss)\n",
    "            valid.append([acc, iter, lr])\n",
    "    \n",
    "    valid_df=pd.DataFrame(valid)\n",
    "    valid_df.columns =['Validation Accuracy', 'Max Iterations', 'Learning Rate']\n",
    "    return valid_df\n",
    "    \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Max Iterations</th>\n",
       "      <th>Learning Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.629213</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.634831</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.640449</td>\n",
       "      <td>2500</td>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.629213</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.662921</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.679775</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.629213</td>\n",
       "      <td>7500</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.679775</td>\n",
       "      <td>7500</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.735955</td>\n",
       "      <td>7500</td>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.629213</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.702247</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.792135</td>\n",
       "      <td>10000</td>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.629213</td>\n",
       "      <td>12500</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.702247</td>\n",
       "      <td>12500</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.814607</td>\n",
       "      <td>12500</td>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.629213</td>\n",
       "      <td>15000</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.702247</td>\n",
       "      <td>15000</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.814607</td>\n",
       "      <td>15000</td>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.629213</td>\n",
       "      <td>17500</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.713483</td>\n",
       "      <td>17500</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.808989</td>\n",
       "      <td>17500</td>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.634831</td>\n",
       "      <td>20000</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.735955</td>\n",
       "      <td>20000</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.808989</td>\n",
       "      <td>20000</td>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.634831</td>\n",
       "      <td>22500</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.735955</td>\n",
       "      <td>22500</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.808989</td>\n",
       "      <td>22500</td>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.634831</td>\n",
       "      <td>25000</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.747191</td>\n",
       "      <td>25000</td>\n",
       "      <td>0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.808989</td>\n",
       "      <td>25000</td>\n",
       "      <td>0.0100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Validation Accuracy  Max Iterations  Learning Rate\n",
       "0              0.629213            2500         0.0001\n",
       "1              0.634831            2500         0.0010\n",
       "2              0.640449            2500         0.0100\n",
       "3              0.629213            5000         0.0001\n",
       "4              0.662921            5000         0.0010\n",
       "5              0.679775            5000         0.0100\n",
       "6              0.629213            7500         0.0001\n",
       "7              0.679775            7500         0.0010\n",
       "8              0.735955            7500         0.0100\n",
       "9              0.629213           10000         0.0001\n",
       "10             0.702247           10000         0.0010\n",
       "11             0.792135           10000         0.0100\n",
       "12             0.629213           12500         0.0001\n",
       "13             0.702247           12500         0.0010\n",
       "14             0.814607           12500         0.0100\n",
       "15             0.629213           15000         0.0001\n",
       "16             0.702247           15000         0.0010\n",
       "17             0.814607           15000         0.0100\n",
       "18             0.629213           17500         0.0001\n",
       "19             0.713483           17500         0.0010\n",
       "20             0.808989           17500         0.0100\n",
       "21             0.634831           20000         0.0001\n",
       "22             0.735955           20000         0.0010\n",
       "23             0.808989           20000         0.0100\n",
       "24             0.634831           22500         0.0001\n",
       "25             0.735955           22500         0.0010\n",
       "26             0.808989           22500         0.0100\n",
       "27             0.634831           25000         0.0001\n",
       "28             0.747191           25000         0.0010\n",
       "29             0.808989           25000         0.0100"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQ8YiE9Y8yuh"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to test on train + validation data\n",
    "frames = [x_train,x_valid]\n",
    "x_train=pd.concat(frames).reset_index(drop=True)\n",
    "y_train=y_train.append(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, bias, loss= train(x_train, y_train, learning_rate=0.01, max_iter=12500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7821229050279329"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions=predict(x_test,weights,bias)\n",
    "accuracy_score(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtkMxBOBWTDm",
    "outputId": "010d3d2f-145a-4ba5-e97b-ca32c0c2b7b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.7027027027027027\n",
      "Precision: 0.7536231884057971\n",
      "False Positive Rate: 0.1619047619047619\n",
      "F Measure: 0.7272727272727273\n"
     ]
    }
   ],
   "source": [
    "fp=0\n",
    "fn=0\n",
    "tp=0\n",
    "tn=0\n",
    "\n",
    "for i in range(len(pred_class)):\n",
    "    if pred_class[i]==1 and y_test[i]==0:\n",
    "        fp+=1\n",
    "    if pred_class[i]==0 and y_test[i]==1:\n",
    "        fn+=1\n",
    "    if pred_class[i]==1 and y_test[i]==1:\n",
    "        tp+=1\n",
    "    if pred_class[i]==0 and y_test[i]==0:\n",
    "        tn+=1\n",
    "\n",
    "recall=tp/(tp+fn)\n",
    "precision=tp/(fp+tp)\n",
    "fpr=fp/(fp+tn)\n",
    "f_meas=(2*recall*precision)/(recall+precision)\n",
    "\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"False Positive Rate:\", fpr)\n",
    "print(\"F Measure:\", f_meas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGqaeWEa9BS3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8100558659217877"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just to compare\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(max_iter=12500)\n",
    "logreg.fit(x_train, y_train)\n",
    "y_pred = logreg.predict(x_test)\n",
    "logreg.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
